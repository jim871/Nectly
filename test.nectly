tokenizer load vocab.txt bpe.txt
input_tokens "ciao mondo"
embedding dim=64
positional sinusoidal
transformer layers=2 heads=4 hidden=64 ffn=256
optimizer adamw lr=0.001 beta1=0.9 beta2=0.999 eps=1e-8 wd=0.01
train small_data.txt epochs=1 batch=2 maxlen=5
generate prompt="ciao" maxlen=5 top_p=0.8 temperature=1.0
